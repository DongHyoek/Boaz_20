{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dacon.io/en/competitions/official/236037/overview/description\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# for graphing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pd.read_csv('./dataset/df_augmented_.csv')\n",
    "train_original.drop(columns=['ID'], inplace=True)\n",
    "test = pd.read_csv('./dataset/test.csv')\n",
    "test.drop(columns=['ID'], inplace=True)\n",
    "submission = pd.read_csv('./datset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습을 진행하는 기기: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':1.6390181782185967e-05,\n",
    "    'TRAIN_BATCH_SIZE': 64,\n",
    "    'TEST_BATCH_SIZE': 128,\n",
    "    'SEED':42\n",
    "}\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps') \n",
    "print('학습을 진행하는 기기:',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, y_train, y_val = train_test_split(train_original, train_original['label'], test_size=0.2, random_state=CFG['SEED'])\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_nm = 'klue/roberta-small'\n",
    "base_model = AutoModel.from_pretrained(model_nm)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log value : 91.48731761060522\n"
     ]
    }
   ],
   "source": [
    "# tokenizer를 통해 각 문장을 토큰화 시킨 리스트 형태로 반환함. \n",
    "# tokenizer는 토큰들이 어디에 저장되었는지 input_ids로 반환.\n",
    "tokenizer_len = [len(tokenizer(s)['input_ids']) for s in train['문장']]\n",
    "sns.histplot(tokenizer_len)\n",
    "plt.show()\n",
    "\n",
    "print(f'log value : {np.mean(tokenizer_len)+3*np.std(tokenizer_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log value : 4.909803820030983\n",
      "original value : 135.61280728232697\n"
     ]
    }
   ],
   "source": [
    "# log를 씌워서 정규분포꼴로 정규화과정.\n",
    "tokenizer_log = np.log(tokenizer_len)\n",
    "sns.histplot(tokenizer_log)\n",
    "plt.show()\n",
    "\n",
    "print(f'log value : {np.mean(tokenizer_log)+3*np.std(tokenizer_log)}')\n",
    "print(f'original value : {np.exp(np.mean(tokenizer_log)+3*np.std(tokenizer_log))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTypeDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, labels=None):\n",
    "        texts = dataframe['문장'].values.tolist()\n",
    "\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=90, truncation=True, return_tensors='pt') for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        if self.labels is not None:\n",
    "            type_tmp = self.labels['type'][idx]\n",
    "            polarity_tmp = self.labels['polarity'][idx]\n",
    "            tense_tmp = self.labels['tense'][idx]\n",
    "            certainty_tmp = self.labels['certainty'][idx]\n",
    "            return text, torch.Tensor(type_tmp), torch.Tensor(polarity_tmp), torch.Tensor(tense_tmp), torch.Tensor(certainty_tmp)\n",
    "        else:\n",
    "            return text, torch.Tensor([-1,-1,-1,-1]), torch.Tensor([-1,-1,-1]), torch.Tensor([-1,-1,-1]), torch.Tensor([-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.klue = base_model # from transformers package\n",
    "\n",
    "        self.fc1 = nn.Linear(768, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.type_clf = nn.Linear(32,4)\n",
    "        self.polarity_clf = nn.Linear(32,3)\n",
    "        self.tense_clf = nn.Linear(32,3)\n",
    "        self.certainty_clf = nn.Linear(32,2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids : token's id / attention_mask : make a model to focus on which token\n",
    "        klue_out = self.klue(input_ids= input_ids, attention_mask = attention_mask)[0][:,0]\n",
    "\n",
    "        x = self.fc1(klue_out)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        type_output = self.type_clf(x)\n",
    "        type_output = self.softmax(type_output)\n",
    "        polarity_output = self.polarity_clf(x)\n",
    "        polarity_output = self.softmax(polarity_output)\n",
    "        tense_output = self.tense_clf(x)\n",
    "        tense_output = self.softmax(tense_output)\n",
    "        certainty_output = self.certainty_clf(x)\n",
    "        certainty_output = self.softmax(certainty_output)\n",
    "\n",
    "        return type_output, polarity_output, tense_output, certainty_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_train(model, train_dataloader, val_dataloader, learning_rate, epochs, model_nm):\n",
    "    best_val_loss = 99999999999999 # setting max (act as infinity)\n",
    "    early_stopping_threshold_count = 0\n",
    "\n",
    "    criterion = {\n",
    "        'type' : nn.CrossEntropyLoss().to(device),\n",
    "        'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "    }\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        model.train() # 모델을 학습모드로\n",
    "        \n",
    "        # 여기가 학습과정\n",
    "        for train_input, type_label, polarity_label, tense_label, certainty_label in tqdm(train_dataloader):\n",
    "            attention_mask = train_input['attention_mask'].to(device)\n",
    "            input_ids = train_input['input_ids'].squeeze(1).to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            type_output, polarity_output, tense_output, certainty_output = model(input_ids, attention_mask) # from the forward function\n",
    "            \n",
    "            loss = 0.25*criterion['type'](type_output, torch.max(type_label.long(), 1)[1]) + \\\n",
    "                    0.25*criterion['polarity'](polarity_output, torch.max(polarity_label.long(), 1)[1]) + \\\n",
    "                    0.25*criterion['tense'](tense_output, torch.max(tense_label.long(), 1)[1]) + \\\n",
    "                    0.25*criterion['certainty'](certainty_output, torch.max(certainty_label.long(), 1)[1])\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # pred와 label을 담을 리스트를 각각 생성\n",
    "        type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "        type_labels, polarity_labels, tense_labels, certainty_labels = [], [], [], []\n",
    "\n",
    "        # 여기가 검증과정\n",
    "        with torch.no_grad(): # since we should not change gradient for validation \n",
    "            total_loss_val = 0\n",
    "            \n",
    "            model.eval() # deactivate training\n",
    "            \n",
    "            # same process as the above\n",
    "            for val_input, vtype_label, vpolarity_label, vtense_label, vcertainty_label in tqdm(val_dataloader):\n",
    "                attention_mask = val_input['attention_mask'].to(device)\n",
    "                input_ids = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                vtype_label = vtype_label.to(device)\n",
    "                vpolarity_label = vpolarity_label.to(device)\n",
    "                vtense_label = vtense_label.to(device)\n",
    "                vcertainty_label = vcertainty_label.to(device)\n",
    "                \n",
    "                # 현재 output은 softmax값인데 이걸 f1 score으로 평가하기 위해선 one-hot encoding 된 라벨값과 비교해야함.\n",
    "                # 단순히 argmax를 사용하게 되면 label encodin처럼 값이 나오므로 one-hot 꼴로 맞춰주기 위한 과정이 아래의 내용.\n",
    "                vtype_output, vpolarity_output, vtense_output, vcertainty_output = model(input_ids, attention_mask) # from the forward function\n",
    "                vtype_output = torch.zeros_like(vtype_output).scatter_(1,torch.argmax(vtype_output,dim = 1).unsqueeze(1),1.)\n",
    "                vpolarity_output = torch.zeros_like(vpolarity_output).scatter_(1,torch.argmax(vpolarity_output,dim = 1).unsqueeze(1),1.)\n",
    "                vtense_output = torch.zeros_like(vtense_output).scatter_(1,torch.argmax(vtense_output,dim = 1).unsqueeze(1),1.)\n",
    "                vcertainty_output = torch.zeros_like(vcertainty_output).scatter_(1,torch.argmax(vcertainty_output,dim = 1).unsqueeze(1),1.)\n",
    "\n",
    "                # 계산을 위해 각 예측값과 정답값들을 저장\n",
    "                type_preds += vtype_output.detach().cpu().numpy().tolist()\n",
    "                type_labels += vtype_label.detach().cpu().numpy().tolist()\n",
    "                \n",
    "                polarity_preds += vpolarity_output.detach().cpu().numpy().tolist()\n",
    "                polarity_labels += vpolarity_label.detach().cpu().numpy().tolist()\n",
    "                \n",
    "                tense_preds += vtense_output.detach().cpu().numpy().tolist()\n",
    "                tense_labels += vtense_label.detach().cpu().numpy().tolist()\n",
    "                \n",
    "                certainty_preds += vcertainty_output.detach().cpu().numpy().tolist()\n",
    "                certainty_labels += vcertainty_label.detach().cpu().numpy().tolist()\n",
    "                \n",
    "                loss = 0.25*criterion['type'](vtype_output, torch.max(vtype_label.long(), 1)[1])+ \\\n",
    "                         0.25*criterion['polarity'](vpolarity_output, torch.max(vpolarity_label.long(), 1)[1]) + \\\n",
    "                         0.25*criterion['tense'](vtense_output, torch.max(vtense_label.long(), 1)[1]) + \\\n",
    "                         0.25*criterion['certainty'](vcertainty_output, torch.max(vcertainty_label.long(), 1)[1])\n",
    "\n",
    "                total_loss_val += loss.item()\n",
    "\n",
    "            type_f1 = f1_score(type_labels, type_preds, average='weighted')\n",
    "            polarity_f1 = f1_score(polarity_labels, polarity_preds, average='weighted')\n",
    "            tense_f1 = f1_score(tense_labels, tense_preds, average='weighted')\n",
    "            certainty_f1 = f1_score(certainty_labels, certainty_preds, average='weighted')\n",
    "            total_f1 = (type_f1 + polarity_f1 + tense_f1 + certainty_f1) / 4\n",
    "            \n",
    "            print(f'---- Epochs: {epoch + 1} ---- \\n ',\n",
    "                  f'| Train Loss: {total_loss_train / len(train_dataloader): .4f} \\n ',\n",
    "                  f'| Val Loss: {total_loss_val / len(val_dataloader): .4f} \\n ',\n",
    "                  f'| Total F1_Score : {total_f1} \\n 유형별 F1_score : {type_f1} \\n 극성별 F1_score : {polarity_f1} \\n 시제별 F1_score : {tense_f1} \\n 확실성별 F1_score : {certainty_f1} ')\n",
    " \n",
    "            if best_val_loss > total_loss_val:\n",
    "              best_val_loss = total_loss_val # saving only the best one\n",
    "              torch.save(model, f\"./{model_nm+ '_best_epoch_'+str(epoch+1)}.pt\")\n",
    "              print(\"Saved model\")\n",
    "              early_stopping_threshold_count = 0\n",
    "            else:\n",
    "              early_stopping_threshold_count += 1 # checking how many epochs have passed that val_loss didn't increase\n",
    "                \n",
    "            if early_stopping_threshold_count >= 3: # ==> patience=1\n",
    "              print(\"Early stopping\")\n",
    "              break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>유형_대화형</th>\n",
       "      <th>유형_사실형</th>\n",
       "      <th>유형_예측형</th>\n",
       "      <th>유형_추론형</th>\n",
       "      <th>극성_긍정</th>\n",
       "      <th>극성_미정</th>\n",
       "      <th>극성_부정</th>\n",
       "      <th>시제_과거</th>\n",
       "      <th>시제_미래</th>\n",
       "      <th>시제_현재</th>\n",
       "      <th>확실성_불확실</th>\n",
       "      <th>확실성_확실</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3년을 맡겨도 금리는 연 1.15∼2.20%로 큰 차이가 없다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>하지만 난관은 지금부터다.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020년 새해 벽두 불쑥 제(화장실) 얘기부터 꺼내 죄송스럽긴 합니다.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>오랜 세월 축적된 기술과 개발 노하우를 가진 미국과 유럽 항공기 제작사들을 짧은 시...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>장애인주차구역에서 차가 나올 때는 장애인이 꼭 탑승하지 않아도 된다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9481</th>\n",
       "      <td>훅을 인수해서 OTT 사업 역량을 내재화한 덕분에 쿠팡플레이를 선보이고 단기간에 ＇...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9482</th>\n",
       "      <td>우리 몸에 들어온 온갖 잡균을 죽여주는 역할을 함은 물론 혈관을 건강하게 해줘 혈관...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9483</th>\n",
       "      <td>고진영(1위), 박성현(2위), 김세영(6위), 이정은(9위), 김효주(12위)에 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9484</th>\n",
       "      <td>제약바이오 업계 A관계자는 ＂복제약인 바이오시밀러가 전체 의약품 수출을 견인한다고 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9485</th>\n",
       "      <td>현재 금액으로만 500억원 이상의 계약을 체결했다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9486 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     문장  유형_대화형  유형_사실형  \\\n",
       "0                   3년을 맡겨도 금리는 연 1.15∼2.20%로 큰 차이가 없다.       0       1   \n",
       "1                                        하지만 난관은 지금부터다.       0       0   \n",
       "2              2020년 새해 벽두 불쑥 제(화장실) 얘기부터 꺼내 죄송스럽긴 합니다.       1       0   \n",
       "3     오랜 세월 축적된 기술과 개발 노하우를 가진 미국과 유럽 항공기 제작사들을 짧은 시...       0       1   \n",
       "4                장애인주차구역에서 차가 나올 때는 장애인이 꼭 탑승하지 않아도 된다.       0       1   \n",
       "...                                                 ...     ...     ...   \n",
       "9481  훅을 인수해서 OTT 사업 역량을 내재화한 덕분에 쿠팡플레이를 선보이고 단기간에 ＇...       1       0   \n",
       "9482  우리 몸에 들어온 온갖 잡균을 죽여주는 역할을 함은 물론 혈관을 건강하게 해줘 혈관...       0       1   \n",
       "9483  고진영(1위), 박성현(2위), 김세영(6위), 이정은(9위), 김효주(12위)에 ...       0       0   \n",
       "9484  제약바이오 업계 A관계자는 ＂복제약인 바이오시밀러가 전체 의약품 수출을 견인한다고 ...       0       1   \n",
       "9485                       현재 금액으로만 500억원 이상의 계약을 체결했다.       0       1   \n",
       "\n",
       "      유형_예측형  유형_추론형  극성_긍정  극성_미정  극성_부정  시제_과거  시제_미래  시제_현재  확실성_불확실  \\\n",
       "0          0       0      0      0      1      0      0      1        0   \n",
       "1          0       1      1      0      0      0      0      1        0   \n",
       "2          0       0      1      0      0      0      0      1        0   \n",
       "3          0       0      1      0      0      0      0      1        0   \n",
       "4          0       0      0      0      1      0      0      1        0   \n",
       "...      ...     ...    ...    ...    ...    ...    ...    ...      ...   \n",
       "9481       0       0      1      0      0      1      0      0        0   \n",
       "9482       0       0      1      0      0      0      0      1        0   \n",
       "9483       0       1      1      0      0      0      1      0        1   \n",
       "9484       0       0      1      0      0      1      0      0        0   \n",
       "9485       0       0      1      0      0      1      0      0        0   \n",
       "\n",
       "      확실성_확실  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "9481       1  \n",
       "9482       1  \n",
       "9483       0  \n",
       "9484       1  \n",
       "9485       1  \n",
       "\n",
       "[9486 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tmp = train[['문장', '유형', '극성', '시제', '확실성']]\n",
    "train_tmp = pd.get_dummies(train_tmp, columns=['유형', '극성', '시제', '확실성'])\n",
    "train_tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음으로는 각 label별로 뽑아서 train_labels에 dictionary형태로 저장해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = train_tmp.iloc[:,1:5].values.tolist()\n",
    "train_polarity = train_tmp.iloc[:,5:8].values.tolist()\n",
    "train_tense = train_tmp.iloc[:,8:11].values.tolist()\n",
    "train_certainty = train_tmp.iloc[:,11:13].values.tolist()\n",
    "train_labels = {\n",
    "    'type': train_type,\n",
    "    'polarity': train_polarity,\n",
    "    'tense': train_tense,\n",
    "    'certainty': train_certainty\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "똑같은 방식으로 validation data도 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tmp = val[['문장', '유형', '극성', '시제', '확실성']]\n",
    "val_tmp = pd.get_dummies(val_tmp, columns=['유형', '극성', '시제', '확실성'])\n",
    "\n",
    "val_type = val_tmp.iloc[:,1:5].values.tolist()\n",
    "val_polarity = val_tmp.iloc[:,5:8].values.tolist()\n",
    "val_tense = val_tmp.iloc[:,8:11].values.tolist()\n",
    "val_certainty = val_tmp.iloc[:,11:13].values.tolist()\n",
    "val_labels = {\n",
    "    'type': val_type,\n",
    "    'polarity': val_polarity,\n",
    "    'tense': val_tense,\n",
    "    'certainty': val_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(SentenceTypeDataset(train_tmp, tokenizer, train_labels), batch_size=CFG['TRAIN_BATCH_SIZE'], shuffle=True, num_workers=0) # num_workers: how many subprocesses to use for data loading  \n",
    "val_dataloader = DataLoader(SentenceTypeDataset(val_tmp, tokenizer, val_labels), batch_size=CFG['TEST_BATCH_SIZE'], num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음으로는 base_model (klue를 이용한 pretrained_model)을 기반으로해서 SentenceClassifier를 불러준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceClassifier(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:58<00:00,  1.25it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 1 ---- \n",
      "  | Train Loss:  0.8340 \n",
      "  | Val Loss:  0.7009 \n",
      "  | Total F1_Score : 0.7734800466059303 \n",
      " 유형별 F1_score : 0.630396729095441 \n",
      " 극성별 F1_score : 0.8643550212785934 \n",
      " 시제별 F1_score : 0.7773695314142177 \n",
      " 확실성별 F1_score : 0.8217989046354689 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:49<00:00,  1.36it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 2 ---- \n",
      "  | Train Loss:  0.7474 \n",
      "  | Val Loss:  0.6735 \n",
      "  | Total F1_Score : 0.8343565600173002 \n",
      " 유형별 F1_score : 0.7354536967027268 \n",
      " 극성별 F1_score : 0.8643550212785934 \n",
      " 시제별 F1_score : 0.8566471223939798 \n",
      " 확실성별 F1_score : 0.8809703996939005 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:50<00:00,  1.34it/s]\n",
      "100%|██████████| 19/19 [00:12<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 3 ---- \n",
      "  | Train Loss:  0.7041 \n",
      "  | Val Loss:  0.6593 \n",
      "  | Total F1_Score : 0.8627778277086937 \n",
      " 유형별 F1_score : 0.8124942105179815 \n",
      " 극성별 F1_score : 0.881631604530694 \n",
      " 시제별 F1_score : 0.8746698592652922 \n",
      " 확실성별 F1_score : 0.8823156365208069 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:57<00:00,  1.27it/s]\n",
      "100%|██████████| 19/19 [00:12<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 4 ---- \n",
      "  | Train Loss:  0.6761 \n",
      "  | Val Loss:  0.6480 \n",
      "  | Total F1_Score : 0.8785203737435774 \n",
      " 유형별 F1_score : 0.8225585725513569 \n",
      " 극성별 F1_score : 0.8976217608455157 \n",
      " 시제별 F1_score : 0.8904590970633394 \n",
      " 확실성별 F1_score : 0.9034420645140975 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:51<00:00,  1.34it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 5 ---- \n",
      "  | Train Loss:  0.6549 \n",
      "  | Val Loss:  0.6413 \n",
      "  | Total F1_Score : 0.8882402139523542 \n",
      " 유형별 F1_score : 0.8371541721383985 \n",
      " 극성별 F1_score : 0.9134034678605099 \n",
      " 시제별 F1_score : 0.8921762818404846 \n",
      " 확실성별 F1_score : 0.9102269339700239 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:49<00:00,  1.36it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 6 ---- \n",
      "  | Train Loss:  0.6393 \n",
      "  | Val Loss:  0.6386 \n",
      "  | Total F1_Score : 0.8931833093817793 \n",
      " 유형별 F1_score : 0.8527338909224654 \n",
      " 극성별 F1_score : 0.9218127505531283 \n",
      " 시제별 F1_score : 0.8890563699249355 \n",
      " 확실성별 F1_score : 0.909130226126588 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:48<00:00,  1.37it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 7 ---- \n",
      "  | Train Loss:  0.6272 \n",
      "  | Val Loss:  0.6365 \n",
      "  | Total F1_Score : 0.8950883286042441 \n",
      " 유형별 F1_score : 0.843769748724261 \n",
      " 극성별 F1_score : 0.9415699649075271 \n",
      " 시제별 F1_score : 0.8939240515661582 \n",
      " 확실성별 F1_score : 0.9010895492190301 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:45<00:00,  1.41it/s]\n",
      "100%|██████████| 19/19 [00:10<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 8 ---- \n",
      "  | Train Loss:  0.6166 \n",
      "  | Val Loss:  0.6340 \n",
      "  | Total F1_Score : 0.899744965101757 \n",
      " 유형별 F1_score : 0.8573406677938453 \n",
      " 극성별 F1_score : 0.9502082076617485 \n",
      " 시제별 F1_score : 0.8911699362962013 \n",
      " 확실성별 F1_score : 0.9002610486552333 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:45<00:00,  1.41it/s]\n",
      "100%|██████████| 19/19 [00:10<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 9 ---- \n",
      "  | Train Loss:  0.6075 \n",
      "  | Val Loss:  0.6332 \n",
      "  | Total F1_Score : 0.9002017796284996 \n",
      " 유형별 F1_score : 0.8452563142735818 \n",
      " 극성별 F1_score : 0.9512091761396309 \n",
      " 시제별 F1_score : 0.8968997458812104 \n",
      " 확실성별 F1_score : 0.907441882219575 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:46<00:00,  1.39it/s]\n",
      "100%|██████████| 19/19 [00:10<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 10 ---- \n",
      "  | Train Loss:  0.6008 \n",
      "  | Val Loss:  0.6320 \n",
      "  | Total F1_Score : 0.9030995567462434 \n",
      " 유형별 F1_score : 0.8529591493169237 \n",
      " 극성별 F1_score : 0.9565880096480789 \n",
      " 시제별 F1_score : 0.8920949699292162 \n",
      " 확실성별 F1_score : 0.9107560980907545 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:45<00:00,  1.41it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 11 ---- \n",
      "  | Train Loss:  0.5952 \n",
      "  | Val Loss:  0.6283 \n",
      "  | Total F1_Score : 0.9066436934712568 \n",
      " 유형별 F1_score : 0.8581168012745475 \n",
      " 극성별 F1_score : 0.9588561943596564 \n",
      " 시제별 F1_score : 0.8943997581190386 \n",
      " 확실성별 F1_score : 0.9152020201317851 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:50<00:00,  1.35it/s]\n",
      "100%|██████████| 19/19 [00:12<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 12 ---- \n",
      "  | Train Loss:  0.5901 \n",
      "  | Val Loss:  0.6291 \n",
      "  | Total F1_Score : 0.9049903410408315 \n",
      " 유형별 F1_score : 0.8536352654590161 \n",
      " 극성별 F1_score : 0.9597013826327145 \n",
      " 시제별 F1_score : 0.8953717461084609 \n",
      " 확실성별 F1_score : 0.9112529699631351 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:53<00:00,  1.31it/s]\n",
      "100%|██████████| 19/19 [00:12<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 13 ---- \n",
      "  | Train Loss:  0.5869 \n",
      "  | Val Loss:  0.6279 \n",
      "  | Total F1_Score : 0.9074683796197037 \n",
      " 유형별 F1_score : 0.8596128423671251 \n",
      " 극성별 F1_score : 0.9613122767783171 \n",
      " 시제별 F1_score : 0.8959722962665975 \n",
      " 확실성별 F1_score : 0.9129761030667752 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:58<00:00,  1.26it/s]\n",
      "100%|██████████| 19/19 [00:12<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 14 ---- \n",
      "  | Train Loss:  0.5833 \n",
      "  | Val Loss:  0.6281 \n",
      "  | Total F1_Score : 0.9072295882261258 \n",
      " 유형별 F1_score : 0.8558577888920703 \n",
      " 극성별 F1_score : 0.9645516117049255 \n",
      " 시제별 F1_score : 0.8944242611441015 \n",
      " 확실성별 F1_score : 0.914084691163406 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:50<00:00,  1.35it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 15 ---- \n",
      "  | Train Loss:  0.5800 \n",
      "  | Val Loss:  0.6261 \n",
      "  | Total F1_Score : 0.9092802932666282 \n",
      " 유형별 F1_score : 0.858537566994577 \n",
      " 극성별 F1_score : 0.9633867617647637 \n",
      " 시제별 F1_score : 0.8978164621754225 \n",
      " 확실성별 F1_score : 0.9173803821317497 \n",
      "Saved model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:49<00:00,  1.37it/s]\n",
      "100%|██████████| 19/19 [00:12<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 16 ---- \n",
      "  | Train Loss:  0.5777 \n",
      "  | Val Loss:  0.6278 \n",
      "  | Total F1_Score : 0.9088376028019363 \n",
      " 유형별 F1_score : 0.8614898404473968 \n",
      " 극성별 F1_score : 0.9636661058723016 \n",
      " 시제별 F1_score : 0.8920178436099152 \n",
      " 확실성별 F1_score : 0.9181766212781315 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:52<00:00,  1.32it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 17 ---- \n",
      "  | Train Loss:  0.5767 \n",
      "  | Val Loss:  0.6296 \n",
      "  | Total F1_Score : 0.9069374985417109 \n",
      " 유형별 F1_score : 0.8577819059608072 \n",
      " 극성별 F1_score : 0.9646565937612858 \n",
      " 시제별 F1_score : 0.8835178895648369 \n",
      " 확실성별 F1_score : 0.9217936048799139 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [01:51<00:00,  1.33it/s]\n",
      "100%|██████████| 19/19 [00:11<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Epochs: 18 ---- \n",
      "  | Train Loss:  0.5745 \n",
      "  | Val Loss:  0.6270 \n",
      "  | Total F1_Score : 0.9084998831426817 \n",
      " 유형별 F1_score : 0.8583796201819699 \n",
      " 극성별 F1_score : 0.9652335420620417 \n",
      " 시제별 F1_score : 0.8937505562533404 \n",
      " 확실성별 F1_score : 0.9166358140733749 \n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_train(model, train_dataloader, val_dataloader, CFG['LEARNING_RATE'], CFG['EPOCHS'], 'kclue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_predictions(model, loader):\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'mps')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    type_probs, polarity_probs, tense_probs, clarity_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_input, _, _, _, _ in tqdm(loader):\n",
    "            attention_mask = data_input['attention_mask'].to(device)\n",
    "            input_ids = data_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "\n",
    "            type_output, polarity_output, tense_output, clarity_output = model(input_ids, attention_mask)\n",
    "            type_probs.append(type_output)\n",
    "            polarity_probs.append(polarity_output)\n",
    "            tense_probs.append(tense_output)\n",
    "            clarity_probs.append(clarity_output)\n",
    "    \n",
    "    return torch.cat(type_probs).cpu().detach().numpy(), \\\n",
    "            torch.cat(polarity_probs).cpu().detach().numpy(), \\\n",
    "            torch.cat(tense_probs).cpu().detach().numpy(), \\\n",
    "            torch.cat(clarity_probs).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"kclue_best_epoch_15.pt\")\n",
    "test_dataloader = DataLoader(SentenceTypeDataset(test, tokenizer), batch_size=CFG['TEST_BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_pred_type, val_pred_polarity, val_pred_tense, val_pred_certainty = get_type_predictions(model, val_dataloader)\n",
    "\n",
    "#val_type = ['대화형' if i==0 else '사실형' if i==1 else '예측형' if i==2 else '추론형' for i in [np.argmax(p) for p in val_pred_type]]\n",
    "#val_polarity = ['긍정' if i==0 else '미정' if i==1 else '부정' for i in [np.argmax(p) for p in val_pred_polarity]]\n",
    "#val_type = ['과거' if i==0 else '미래' if i==1 else '현재' for i in [np.argmax(p) for p in val_pred_tense]]\n",
    "#val_type = ['불확실' if i==0 else '확실' for i in [np.argmax(p) for p in val_pred_certainty]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:36<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_type, test_pred_polarity, test_pred_tense, test_pred_certainty = get_type_predictions(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_type = ['대화형' if i==0 else '사실형' if i==1 else '예측형' if i==2 else '추론형' for i in [np.argmax(p) for p in test_pred_type]]\n",
    "test_polarity = ['긍정' if i==0 else '미정' if i==1 else '부정' for i in [np.argmax(p) for p in test_pred_polarity]]\n",
    "test_tense = ['과거' if i==0 else '미래' if i==1 else '현재' for i in [np.argmax(p) for p in test_pred_tense]]\n",
    "test_certainty = ['불확실' if i==0 else '확실' for i in [np.argmax(p) for p in test_pred_certainty]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_sum = []\n",
    "for i in range(len(test_type)):\n",
    "    label_sum.append(f'{test_type[i]}-{test_polarity[i]}-{test_tense[i]}-{test_certainty[i]}')\n",
    "\n",
    "submission['label'] = pd.Series(label_sum)\n",
    "#submission.to_csv('./result/vocab + augment + optuna.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 자막 데이터셋에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Korpora] Corpus `open_subtitles` is already installed at C:\\Users\\A\\Korpora\\open_subtitles\\en-ko.tmx.gz\n",
      "[Korpora] Corpus `open_subtitles` is already installed at C:\\Users\\A\\Korpora\\open_subtitles\\en-ko.tmx\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : TRAC (https://trac.edgewall.org/)\n",
      "    Repository : http://opus.nlpl.eu/OpenSubtitles-v2018.php\n",
      "    References :\n",
      "        - P. Lison and J. Tiedemann, 2016, OpenSubtitles2016: Extracting Large Parallel Corpora\n",
      "          from Movie and TV Subtitles. In Proceedings of the 10th International Conference on\n",
      "          Language Resources and Evaluation (LREC 2016)\n",
      "\n",
      "    This is a new collection of translated movie subtitles from http://www.opensubtitles.org/.\n",
      "\n",
      "    [[ IMPORTANT ]]\n",
      "    If you use the OpenSubtitle corpus: Please, add a link to http://www.opensubtitles.org/\n",
      "    to your website and to your reports and publications produced with the data!\n",
      "    I promised this when I got the data from the providers of that website!\n",
      "\n",
      "    This is a slightly cleaner version of the subtitle collection using improved sentence alignment\n",
      "    and better language checking.\n",
      "\n",
      "    62 languages, 1,782 bitexts\n",
      "    total number of files: 3,735,070\n",
      "    total number of tokens: 22.10G\n",
      "    total number of sentence fragments: 3.35G\n",
      "\n",
      "    [[ NOTICE ]]\n",
      "    In original data, the source language is `en` and target language is `ko`. However in Korpora,\n",
      "    we change the language pair so that source language is `ko` and target language is `en`.\n",
      "\n",
      "    # License\n",
      "    Open Data. Details in https://opendefinition.org/od/2.1/en/\n",
      "\n",
      "[Korpora] Corpus `open_subtitles` is already installed at C:\\Users\\A\\Korpora\\open_subtitles\\en-ko.tmx.gz\n",
      "[Korpora] Corpus `open_subtitles` is already installed at C:\\Users\\A\\Korpora\\open_subtitles\\en-ko.tmx\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch('open_subtitles')\n",
    "movie_set = Korpora.load('open_subtitles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1269683, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>황새 아저씨를 기다리세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>찾아와 선물을 주실 거예요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>가난하든 부자이든 상관이 없답니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>왜 그런 생각을 못했지?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>네 귀를 좀 봐, 덤보!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>웬만한 날개보다도 크잖아</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>옛날엔 애물 단지였지만 이젠 보물 단지가 될 거야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>이제야 확실히 알겠어</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    문장\n",
       "0    폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...\n",
       "1                 우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!\n",
       "2                                        황새 아저씨를 기다리세요\n",
       "3                                       찾아와 선물을 주실 거예요\n",
       "4                                   가난하든 부자이든 상관이 없답니다\n",
       "..                                                 ...\n",
       "495                                      왜 그런 생각을 못했지?\n",
       "496                                      네 귀를 좀 봐, 덤보!\n",
       "497                                      웬만한 날개보다도 크잖아\n",
       "498                        옛날엔 애물 단지였지만 이젠 보물 단지가 될 거야\n",
       "499                                        이제야 확실히 알겠어\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임 생성 -> 너무 많아서 500개 까지만 진행\n",
    "movie_df = pd.DataFrame(movie_set.get_all_texts(),columns = ['문장'])\n",
    "print(movie_df.shape)\n",
    "movie_df = movie_df.iloc[:500,:]\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"kclue_best_epoch_15.pt\") # 성능이 제일 좋았던 (내꺼 기준 augmentation tunning 모델 불러옴)\n",
    "test_dataloader = DataLoader(SentenceTypeDataset(movie_df, tokenizer), batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "our_test_pred_type, our_test_pred_polarity, our_test_pred_tense, our_test_pred_certainty = get_type_predictions(model, test_dataloader) # 모델 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 형식에 맞게 변환해주고\n",
    "our_test_type = ['대화형' if i==0 else '사실형' if i==1 else '예측형' if i==2 else '추론형' for i in [np.argmax(p) for p in our_test_pred_type]]\n",
    "our_test_polarity = ['긍정' if i==0 else '미정' if i==1 else '부정' for i in [np.argmax(p) for p in our_test_pred_polarity]]\n",
    "our_test_tense = ['과거' if i==0 else '미래' if i==1 else '현재' for i in [np.argmax(p) for p in our_test_pred_tense]]\n",
    "our_test_certainty = ['불확실' if i==0 else '확실' for i in [np.argmax(p) for p in our_test_pred_certainty]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label형식에 맞게 다시 합쳐주고\n",
    "label_sum = []\n",
    "for i in range(len(our_test_type)):\n",
    "    label_sum.append(f'{our_test_type[i]}-{our_test_polarity[i]}-{our_test_tense[i]}-{our_test_certainty[i]}')\n",
    "\n",
    "# submission 저장.\n",
    "movie_df['label'] = label_sum\n",
    "#movie_df.to_csv('./inference/movie_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!</td>\n",
       "      <td>대화형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>황새 아저씨를 기다리세요</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>찾아와 선물을 주실 거예요</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>가난하든 부자이든 상관이 없답니다</td>\n",
       "      <td>대화형-부정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>백만장자도 하나 가난뱅이도 하나</td>\n",
       "      <td>사실형-긍정-현재-불확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>황새 아저씨를 기다리세요</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>도망쳐도 소용없어요 반드시 찾아내니까요</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>세상 끝에 있어도 하늘 꼭대기에 있어도</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>황새 아저씨는 찾아간답니다</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  문장          label\n",
       "0  폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘...   사실형-긍정-현재-확실\n",
       "1               우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!   대화형-부정-현재-확실\n",
       "2                                      황새 아저씨를 기다리세요   대화형-긍정-현재-확실\n",
       "3                                     찾아와 선물을 주실 거예요   대화형-긍정-현재-확실\n",
       "4                                 가난하든 부자이든 상관이 없답니다   대화형-부정-현재-확실\n",
       "5                                  백만장자도 하나 가난뱅이도 하나  사실형-긍정-현재-불확실\n",
       "6                                      황새 아저씨를 기다리세요   대화형-긍정-현재-확실\n",
       "7                              도망쳐도 소용없어요 반드시 찾아내니까요   대화형-긍정-현재-확실\n",
       "8                              세상 끝에 있어도 하늘 꼭대기에 있어도   사실형-긍정-현재-확실\n",
       "9                                     황새 아저씨는 찾아간답니다   대화형-긍정-현재-확실"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!</td>\n",
       "      <td>대화형-긍정-현재-불확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>황새 아저씨를 기다리세요</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>찾아와 선물을 주실 거예요</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>가난하든 부자이든 상관이 없답니다</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>황새 아저씨를 기다리세요</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>내가 바보였어</td>\n",
       "      <td>대화형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>왜 그런 생각을 못했지?</td>\n",
       "      <td>대화형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>네 귀를 좀 봐, 덤보!</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>웬만한 날개보다도 크잖아</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>이제야 확실히 알겠어</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       문장          label\n",
       "1    우리의 한결같은 심부름꾼 황새 아저씨 가는 길을 그 누가 막으랴!  대화형-긍정-현재-불확실\n",
       "2                           황새 아저씨를 기다리세요   대화형-긍정-현재-확실\n",
       "3                          찾아와 선물을 주실 거예요   대화형-긍정-현재-확실\n",
       "4                      가난하든 부자이든 상관이 없답니다   대화형-긍정-현재-확실\n",
       "6                           황새 아저씨를 기다리세요   대화형-긍정-현재-확실\n",
       "..                                    ...            ...\n",
       "494                               내가 바보였어   대화형-긍정-과거-확실\n",
       "495                         왜 그런 생각을 못했지?   대화형-긍정-과거-확실\n",
       "496                         네 귀를 좀 봐, 덤보!   대화형-긍정-현재-확실\n",
       "497                         웬만한 날개보다도 크잖아   대화형-긍정-현재-확실\n",
       "499                           이제야 확실히 알겠어   대화형-긍정-현재-확실\n",
       "\n",
       "[315 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df[movie_df['label'].str.contains(\"대화형\") == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "대화형-긍정-현재-확실     223\n",
       "추론형-긍정-현재-확실      98\n",
       "사실형-긍정-현재-확실      75\n",
       "대화형-긍정-현재-불확실     51\n",
       "대화형-긍정-과거-확실      27\n",
       "대화형-미정-현재-불확실      6\n",
       "추론형-긍정-미래-확실       5\n",
       "대화형-긍정-미래-확실       5\n",
       "사실형-긍정-과거-확실       3\n",
       "대화형-긍정-과거-불확실      3\n",
       "추론형-긍정-미래-불확실      2\n",
       "추론형-긍정-현재-불확실      1\n",
       "사실형-긍정-미래-확실       1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df['label'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 혐오댓글 데이터셋 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/dev.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/test.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/train.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/labeled/dev.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/labeled/train.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/test.no_label.tsv\n",
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Authors :\n",
      "        - Jihyung Moon* (inmoonlight@github)\n",
      "        - Won Ik Cho* (warnikchow@github)\n",
      "        - Junbum Lee (beomi@github)\n",
      "        * equal contribution\n",
      "    Repository : https://github.com/kocohub/korean-hate-speech\n",
      "    References :\n",
      "        - Moon, J., Cho, W. I., & Lee, J. (2020). BEEP! Korean Corpus of Online News\n",
      "          Comments for Toxic Speech Detection. arXiv preprint arXiv:2005.12503.\n",
      "\n",
      "    We provide the first human-annotated Korean corpus for toxic speech detection and the large unlabeled corpus.\n",
      "    The data is comments from the Korean entertainment news aggregation platform.\n",
      "\n",
      "    # License\n",
      "    Creative Commons Attribution-ShareAlike 4.0 International License.\n",
      "    Visit here for detail : https://creativecommons.org/licenses/by-sa/4.0/\n",
      "\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/unlabeled/unlabeled_comments_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_1.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_2.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_3.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_4.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/unlabeled_comments.news_title_5.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/dev.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/test.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/news_title/train.news_title.txt\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/labeled/dev.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/labeled/train.tsv\n",
      "[Korpora] Corpus `korean hate speech` is already installed at /root/Korpora/korean_hate_speech/test.no_label.tsv\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch('korean_hate_speech')\n",
    "hate_set = Korpora.load('korean_hate_speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2043234, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-db19d164-3e03-4f2b-bc05-4b00b537d514\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>지현우 나쁜놈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>설마 ㅈ 현정 작가 아니지??</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>한효주가 뭐가 아쉬워서 사람 많은 클럽에서 침까지 질질 흘리며 콧물까지흘리며 마약을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>윤정언니 힘내세요~ 앞으로 꽃길만걷기를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>페미들 ㄹㅇ 토나온다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>안타깝네요ㅜㅜ좋아하던 배우인데ㅜ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>그래 자랑이다. 개독이 얼마나 무서운데 얼마나 갈지 함 두고보자.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db19d164-3e03-4f2b-bc05-4b00b537d514')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-db19d164-3e03-4f2b-bc05-4b00b537d514 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-db19d164-3e03-4f2b-bc05-4b00b537d514');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                    문장\n",
       "0                          송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.\n",
       "1                                              지현우 나쁜놈\n",
       "2           알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라\n",
       "3                                     설마 ㅈ 현정 작가 아니지??\n",
       "4    이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...\n",
       "..                                                 ...\n",
       "495  한효주가 뭐가 아쉬워서 사람 많은 클럽에서 침까지 질질 흘리며 콧물까지흘리며 마약을...\n",
       "496                           윤정언니 힘내세요~ 앞으로 꽃길만걷기를...\n",
       "497                                        페미들 ㄹㅇ 토나온다\n",
       "498                                  안타깝네요ㅜㅜ좋아하던 배우인데ㅜ\n",
       "499               그래 자랑이다. 개독이 얼마나 무서운데 얼마나 갈지 함 두고보자.\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hate_df = pd.DataFrame(hate_set.get_all_texts(),columns = ['문장'])\n",
    "print(hate_df.shape)\n",
    "hate_df = hate_df.iloc[:500,:]\n",
    "hate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./kclue_epoch_12_tunning.pt\") # 성능이 제일 좋았던 (내꺼 기준 augmentation tunning 모델 불러옴)\n",
    "test_dataloader = DataLoader(SentenceTypeDataset(hate_df, tokenizer), batch_size=128, shuffle=False)\n",
    "# test_dataloader = DataLoader(SentenceTypeDataset(test, tokenizer), batch_size=CFG['BATCH_SIZE'], shuffle=False) # DataLoader로 구성하고, Shuffle은 무조건 False!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da23a04d1264876804ef43226ff6f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "our_test_pred_type, our_test_pred_polarity, our_test_pred_tense, our_test_pred_certainty = get_type_predictions(model, test_dataloader) # 모델 inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 형식에 맞게 변환해주고\n",
    "our_test_type = ['대화형' if i==0 else '사실형' if i==1 else '예측형' if i==2 else '추론형' for i in [np.argmax(p) for p in our_test_pred_type]]\n",
    "our_test_polarity = ['긍정' if i==0 else '미정' if i==1 else '부정' for i in [np.argmax(p) for p in our_test_pred_polarity]]\n",
    "our_test_tense = ['과거' if i==0 else '미래' if i==1 else '현재' for i in [np.argmax(p) for p in our_test_pred_tense]]\n",
    "our_test_certainty = ['불확실' if i==0 else '확실' for i in [np.argmax(p) for p in our_test_pred_certainty]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label형식에 맞게 다시 합쳐주고\n",
    "label_sum = []\n",
    "for i in range(len(our_test_type)):\n",
    "    label_sum.append(f'{our_test_type[i]}-{our_test_polarity[i]}-{our_test_tense[i]}-{our_test_certainty[i]}')\n",
    "\n",
    "# submission 저장.\n",
    "hate_df['label'] = label_sum\n",
    "# hate_df.to_csv('./inference/hate_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d7f28e74-06ee-4148-b5e7-93406ed3b16a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.</td>\n",
       "      <td>추론형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>지현우 나쁜놈</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>설마 ㅈ 현정 작가 아니지??</td>\n",
       "      <td>대화형-부정-현재-불확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>한효주가 뭐가 아쉬워서 사람 많은 클럽에서 침까지 질질 흘리며 콧물까지흘리며 마약을...</td>\n",
       "      <td>대화형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>윤정언니 힘내세요~ 앞으로 꽃길만걷기를...</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>페미들 ㄹㅇ 토나온다</td>\n",
       "      <td>추론형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>안타깝네요ㅜㅜ좋아하던 배우인데ㅜ</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>그래 자랑이다. 개독이 얼마나 무서운데 얼마나 갈지 함 두고보자.</td>\n",
       "      <td>대화형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d7f28e74-06ee-4148-b5e7-93406ed3b16a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d7f28e74-06ee-4148-b5e7-93406ed3b16a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d7f28e74-06ee-4148-b5e7-93406ed3b16a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                    문장          label\n",
       "0                          송중기 시대극은 믿고본다. 첫회 신선하고 좋았다.   추론형-긍정-과거-확실\n",
       "1                                              지현우 나쁜놈   사실형-긍정-현재-확실\n",
       "2           알바쓰고많이만들면되지 돈욕심없으면골목식당왜나온겨 기댕기게나하고 산에가서팔어라   대화형-긍정-현재-확실\n",
       "3                                     설마 ㅈ 현정 작가 아니지??  대화형-부정-현재-불확실\n",
       "4    이미자씨 송혜교씨 돈이 그리 많으면 탈세말고 그돈으로 평소에 불우이웃에게 기부도 좀...   대화형-긍정-현재-확실\n",
       "..                                                 ...            ...\n",
       "495  한효주가 뭐가 아쉬워서 사람 많은 클럽에서 침까지 질질 흘리며 콧물까지흘리며 마약을...   대화형-긍정-과거-확실\n",
       "496                           윤정언니 힘내세요~ 앞으로 꽃길만걷기를...   대화형-긍정-현재-확실\n",
       "497                                        페미들 ㄹㅇ 토나온다   추론형-긍정-현재-확실\n",
       "498                                  안타깝네요ㅜㅜ좋아하던 배우인데ㅜ   대화형-긍정-현재-확실\n",
       "499               그래 자랑이다. 개독이 얼마나 무서운데 얼마나 갈지 함 두고보자.   대화형-긍정-현재-확실\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hate_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "90e68e2d215918e27054de83af9e76c00ba81faef2a1fcd4e4ecb816270b0b60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
